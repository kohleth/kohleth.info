---
title: Sample size calculation for variance component estimation
author: Kohleth Chia
date: '2017-10-04'
categories:
  - r
tags:
  - linear mixed model
slug: sample-size-calculation-for-variance-component-estimation
cover: /post_imgs/sampleSizeVcomp.png
---
```{r,include=FALSE}
knitr::opts_chunk$set(warning=FALSE,message=FALSE,collapse=TRUE,echo=TRUE,prompt=TRUE)
```
Some time ago my boss came to me with a rather uncommon sample size calculation question. The situation is as such:

My boss and his group of engineers are investigating their own variability / repeatability in a common task that they would perform everyday. This task involves them taking / making certain measurements of a human subject on both sides (left & right). So they have found 23 subjects, and two engineers have already taken / made the measurements with 2 replicates each. So ideally this would be 23 subjects X 2 sides X 2 engineers X 2 reps = 184 measurements. But in reality, certain engineer-subject combination did not happen, so it was an imbalanced design with 180 measurements only. 

So far, this is nothing but a simple linear mixed model exercise, with the inter- and intra- engineer (assessor) variability being the statistics of interest.

But now comes the interesting part. My boss was the third engineer who needed to do those measurements. But he was a busy person, and he was also the manager of that group of engineers. i.e. He had a bit of power. So naturally he asked the question, 'do I need to examine all 23 subjects?'

Ok, in my statistical view, the question is then what is the minimal number of subjects my boss would need to examine? Obviously, we need some sort of stopping criterion. I knew this must have something to do with the precision of the estimated variance component, but I didn't know what exactly our target should be. So let's just do some simulation and see what we get.

Knowing that estimated variance components are <a href="//bbolker.github.io/mixedmodels-misc/glmmFAQ.html#standard-errors-of-variance-estimates" target="_blank" rel="noopener noreferrer">rarely symmetrically distributed</a>, I decided to use the width of their 95% parametric bootstrap C.I. as a measure of uncertainty (precision).

And bootstrap turns out to be particularly suitable here, because in the bootstrap world (i.e. my resamples) I can add synthetic data which represent the hypothetical measurements my boss would take. So the procedure is clear now, I just keep adding synthetic data and compute the bootstrap C.I. width, then eventually we will get a curve of C.I. width against number of extra measurements taken, which will then inform our sample size decision.

I have anonymize the data so you can have a look.

```{r,cache=T,echo=FALSE}
library(kableExtra)
dummy=readRDS("../post_data/dummy.rds")
knitr::kable(dummy, "html") %>%
  kable_styling() %>%
  scroll_box(width = "500px", height = "200px")
```

Let's load some libraries.

```{r,message=FALSE,warning=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(lme4)
B=1000 ## number of bootstrap resample
```

The next step is to fit a mixed model to the existing data generated by the two engineers.^[Two things to explain about the fitted model. 1. Strictly speaking we should add the term `(1|Subject:Assessor:Side)` into the model. I did that, but it wasn't returning any positive variance component estimate, so I just removed it from the model (which is the same as lump it together with the residual term). 2. I used `use.u=TRUE` in the model which forces it to *not* re-estimate (or re-predict) the random Assessor effect (the BLUPs) for the 2 engineers who already did their assessment. I think this is the correct choice because in reality, their BLUPs will not be re-estimated/re-predicted.] I also computed the confidence interval of the inter-assessor variance component to get an idea of the variability.

```{r,cache=T,message=FALSE,warning=FALSE}
lmm=lmer(Y~1+(1|Subject/Side)+(1|Assessor)+(1|Subject:Assessor),data=dummy)
confint0=confint(lmm,method="boot",oldNames=FALSE,nsim=B,use.u=TRUE)
confint0["sd_(Intercept)|Assessor",]
```

Next, I set up the parametric bootstrap. Usually, bootstrap procedure resample to the same size as the original dataset. But this time, I actually need to over-resample to incorporate the synthetic data. Note that in parametric bootstrap the response variable of the synthetic data is irrelevant, which is fantastic as that is what I do not have! 
```{r,cache=T,message=FALSE,warning=FALSE}
X0=model.frame(lmm) ## original model frame
nsList=seq(1,length(unique(dummy$Subject)),by=3)  ## test point
set.seed(123)
## parametric bootstrap
res=lapply(nsList,function(ns){
  bossX=subset(X0,Assessor=="A1"&as.numeric(Subject)<=ns)%>%
    mutate(Assessor="Boss",Y=NA) ## Y=NA: synthetic data is hypothetical, no real measurments were taken.
  X1=X0%>%bind_rows(bossX)  ## append new rows (synthetic data) from Boss's hypothetical assessment
  simY=simulate(lmm,use.u=TRUE,newdata=X1,allow.new.levels=TRUE,nsim=B) ## simulate response variable
  lmeri=lmer(simY[,1]~1+(1|Subject/Side)+(1|Assessor)+(1|Subject:Assessor),data=X1) ## fit the model once, so I can use `refit` below.
  boot=sapply(1:B,function(j){
    mod=refit(lmeri,newresp=simY[,j]) ## refit the model with new simulated response
    intra=sigma(mod) ## intra-assessor standard deviation
    inter=attr(VarCorr(mod)$Assessor,"stddev") ## inter-assessor standard deviation
    c(intra=intra,inter=inter)
  })
})
```

After the bootstrap resample I simply use the percentile method to calculate the 95% C.I. width.

```{r,message=FALSE, warning=FALSE}
bres=sapply(res,function(r){
  apply(r,1,quantile,p=c(0.025,0.975))%>%
    apply(2,diff)  # diff turns the C.I. into width.
}) 
```

Finally, let's plot it.
```{r}
data_frame(nSubject=nsList)%>%
  cbind(t(bres))%>%
  setNames(c("nSubject","intra","inter"))%>%
  gather(var,CI.width,-nSubject)%>%
  ggplot(aes(x=nSubject,y=CI.width,col=var))+
  geom_line()+
  geom_point()+
  geom_hline(yintercept = diff(confint0["sd_(Intercept)|Assessor",]),lty=2,col="#F8766D")+
  geom_hline(yintercept = diff(confint0["sigma",]),lty=2,col="#00BFC4")+
  expand_limits(y=0)+
  scale_x_continuous(breaks = seq(0,23,by=2))+
  annotate("text",4,1.1,label="inter-Assessor",col="#F8766D")+
  annotate("text",3,0.6,label="intra-Assessor",col="#00BFC4")+
  theme(legend.position = "none")+
  labs(title="95% C.I. width vs. number of subjects assessed by my Boss")
```

The two dotted lines are the current width of the respective confidence interval. We can observe from the graph that if my boss examine at least 4 subjects, the width of the confidence interval of the estimated variance component will be about the same as the current one. But of course, we need to be a bit conservative about this because, at least, the simulation is done conditionally on the estimated parameters, so in reality we will probably get more variability (thus wider confidence interval).

## Final Remarks
1. This is a slightly unusual sample size calculation request because usually, the objective is to accurately estimate some real treatment effect, rather than variance components.
2. Parametric bootstrap is surprisingly useful here because i) it allows us to over-sample as much data point as we like, to account for the hypothetical extra assessment that would be done by my boss. ii) it does not require us to know the response variable of the synthetic data which is just what we needed.
3. I think technically I can solve this algebraically using information matrices. But bootstrap (and simulation in general) is just much easier.