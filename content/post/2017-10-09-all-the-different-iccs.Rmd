---
title: All the different ICCs
author: Kohleth Chia
date: '2017-10-09'
slug: all-the-different-iccs
description: How many types of ICC do you know? I tried to re-derive all of them from the estimated variance components of a linear mixed model.
categories: []
tags: []
draft: true
---

```{r,include=FALSE}
knitr::opts_chunk$set(warning=FALSE,message=FALSE,collapse=TRUE,echo=TRUE,prompt=TRUE)
```
I recently came across <a href=//www.aliquote.org/cours/2012_biomed/biblio/Shrout1979.pdf target="_blank">Shrout and Fleiss (1979)</a> which described 6 different kinds of Intra-Class-Correlation (ICC): ICC(1), ICC(2), ICC(3), ICC(1,k), ICC(2,k), ICC(3,k).^[this naming is as ~~un~~informative as the type-X ANOVA naming-- if you have been working with this, you will know it by heart. But for others, it doesn\'t really mean much.]

[Shrout and Fleiss (1979)]:http://

Turns out, the `pysch` package has the function `ICC` which calculates all this for you.  It even gives you p-value and lower/upper boud. Pretty handy (if you know what you are doing).

```{r psych, message=FALSE,warning=FALSE,cache=T}
## their demo. see ?psych::ICC
library(psych)
sf <- matrix(c(9,    2,   5,    8,
               6,    1,   3,    2,
               8,    4,   6,    8,
               7,    1,   2,    6,
               10,   5,   6,    9,
               6,   2,   4,    7),ncol=4,byrow=TRUE)
colnames(sf) <- paste("J",1:4,sep="")
rownames(sf) <- paste("S",1:6,sep="")
sf  #example from Shrout and Fleiss (1979)
ICC(sf)
```

Usually, if I want an ICC, I would calculate it manually from the estimated variance components of a linear (mixed) model. So let\'s see if I can get my calculation to match it.

### ICC(1)
From the description in the help file `?psych::ICC`,

>   Each target is rated by a different judge and the judges are selected at random. (This is a one-way ANOVA fixed effects model and is found by (MSB- MSW)/(MSB+ (nr-1)*MSW))

So the variation between judges is lumped together with that of the error term. That implies the model

$$ s_{ij}=subject_i+e_{ij}$$
where \$s_{ij}\$ is the score of subject \$i\$ given by judge \$j\$. In here and the following, unless stated otherwise, we assume all terms are random. While this is different to the 'fixed effects model' stated in the description, I believe for balanced design the calculation involving variance components will be the same, and for unbalanced design the ANOVA approach wouldn't work anyway.

Therefore, for ICC(1), we get 
$$ 
\begin{align}
corr(s_{ij},s_{ij'})&=\frac{var(s_{ij})+var(s_{ij'})-var(s_{ij}-s_{ij'})}{2var(s_{ij})}\\
&=\sigma^2_{subject}/(\sigma^2_{subject}+\sigma^2_e)
\end{align}
$$


### ICC(2)

>  A random sample of k judges rate each target. The measure is one of absolute agreement in the ratings. Found as (MSB- MSE)/(MSB + (nr-1)*MSE + nr*(MSJ-MSE)/nc).

This assumes the model $$ s_{ij}=subject_i+judge_j+e_ij$$.

And leads to ICC(2) being
$$
\begin{align}
corr(s_{ij},s_{ij'})=\sigma^2_{subject}/(\sigma^2_{subject}+\sigma^2_{judge}+\sigma^2_e)
\end{align}
$$

### ICC(3)

> A fixed set of k judges rate each target. There is no generalization to a larger population of judges. (MSB - MSE)/(MSB+ (nr-1)*MSE).

This assumes the model  $$ s_{ij}=subject_i+judge'_j+e_ij$$
where \$judge'_j\$ is a fixed effect term.

So ICC(3) is
$$
corr(s_{ij},s_{ij'})=\sigma^2_{subject}/(\sigma^2_{subject}+\sigma^2_e)
$$

Although the above formula is the same as that for ICC(1), the fact that \$judge'_j\$ is accounted for in the model means the calculated ICC will be different.

### ICC(.,k)
According to the description,

> Then, for each of these cases, is reliability to be estimated for a single rating or for the average of k ratings? (The 1 rating case is equivalent to the average intercorrelation, the k rating case to the Spearman Brown adjusted reliability.)

So, the assumed model stays the same as before. But the correlation is calculated based on the rater-averaged variable.

### ICC(1,k)
Let's first calculate the building blocks.
$$
\begin{align}
s_{i.}&=subject_i+e_i. \\
&=subject_i+\sum_j e_{ij}/J
\end{align}
$$

$$var(s_{i.})=\sigma_{subject}+\sigma^2_e/J$$
$$s_{i.}-s_{i.'}=\sum_j e_{ij}/J-\sum_{j'} e_{ij'}/J'$$
$$var(s_{i.}-s_{i.'})=2\sigma^2_e/J$$
So, ICC(1,k) equals
$$
\begin{align}
corr(s_{i.},s_{i.'})&=\frac{var(s_{i.})+var(s_{i.'})-var(s_{i.}-s_{i.'})}{2var(s_{i.})}\\
&=\frac{\sigma^2_{subject}}{\sigma^2_{subject}+\sigma^2_e/J}
\end{align}
$$

### ICC(2,k)

Similar derivation with model $$s_i.=subject_i+\sum_j judge_j/J+\sum_j e_{ij}/J$$ leads to 
$$
\begin{align}
corr(s_{i.},s_{i.'})&=\frac{\sigma^2_{subject}}{\sigma^2_{subject}+\sigma^2_{judge}/J+\sigma^2_e/J}
\end{align}
$$

### ICC(3,k)

Finally, ICC(3,k) is
$$
\begin{align}
corr(s_{i.},s_{i.'})=\frac{\sigma^2_{subject}}{\sigma^2_{subject}+\sigma^2_e/J}
\end{align}
$$

which, again, despite the same form as ICC(1,k), the fact that \$\\sum_j judge_j/J\$ is accounted for in the model would make the answer different.

## R implementation

```{r,message=FALSE, warning=FALSE}
## turn sf into a 'tidy' dataframe
library(dplyr);library(tidyr);library(tibble);library(lme4);library(boot)
sfDF=sf%>%
  as.data.frame%>%
  rownames_to_column("subject")%>%
  gather(judge,score,J1:J4)
```

```{r ICC_fn,message=FALSE,warning=FALSE,cache=T}
ICC_lmm=function(dat){
  m1=lmer(score~(1|subject),data=dat)
  m2=lmer(score~(1|subject)+(1|judge),data=dat)
  m3=lmer(score~(1|subject)+judge,data=dat)

  J=length(unique(dat$judge))

  ICC1=function(mod){
    v=as.data.frame(VarCorr(mod))
    s2subj=v[v$grp=="subject","vcov"]
    s2res=v[v$grp=="Residual","vcov"]
    c(ICC1=s2subj/(s2subj+s2res),
      ICC1k=s2subj/(s2subj+s2res/J))
  }
  
  ICC2=function(mod){
    v=as.data.frame(VarCorr(mod))
    s2subj=v[v$grp=="subject","vcov"]
    s2judg=v[v$grp=="judge","vcov"]
    s2res=v[v$grp=="Residual","vcov"]
    c(ICC2=s2subj/sum(v[,"vcov"]),
      ICC2k=s2subj/(s2subj+(s2judg+s2res)/J))
  }
  
  ICC3=function(mod){
    icc=ICC1(mod)  ## but the input model should be different
    names(icc)=c("ICC3","ICC3k")
    icc
  }
 
  ICCs=c(ICC1(m1),ICC2(m2),ICC3(m3))
  
  B=500
  ICCboot=list(ICC1=bootMer(m1,ICC1,nsim = B),
              ICC2=bootMer(m2,ICC2,nsim = B),
              ICC3=bootMer(m3,ICC3,nsim = B))
  
  calcBootCI=function(b,ind=1){
    boot.ci(b,type="perc",index=ind)$percent[4:5]
  }
  
  out=list(ICC1=calcBootCI(ICCboot$ICC1),
       ICC2=calcBootCI(ICCboot$ICC2),
       ICC3=calcBootCI(ICCboot$ICC3),
       ICC1k=calcBootCI(ICCboot$ICC1,2),
       ICC2k=calcBootCI(ICCboot$ICC2,2),
       ICC3k=calcBootCI(ICCboot$ICC3,2))%>%
    do.call(rbind,.)
  
  out=data.frame(ICCs[c("ICC1","ICC2","ICC3","ICC1k","ICC2k","ICC3k")],out)
  colnames(out)=c("val","lower","upper")
  out
}
```

```{r, warning=FALSE, message=FALSE, cache=T,dependson='ICC_fn'}
ICC_lmm(sfDF)
```

The point estimate matches with the output of `psych::ICC` but our intervals are a bit different. This is expected as I `psych::ICC` uses the F-distribution, whereas in `ICC_lmm` I used percentile bootstrap C.I.

One benefit of using a linear mixed model is that it can handle imbalanced design.

So suppose we delete one observation,
```{r,collapse=T}
sf_imb=sf
sf_imb[3,3]=NA
sf_imb
```

```{r, message=F,warning=F, cache=T,error=T}
psych::ICC(sf_imb) ## fails
```


There is an argument `missing=FALSE` in `psych::ICC`, but if you inspect the inner code, the `aov` function will warning against an unbalanced design.

```{r,message=FALSE,warning=FALSE,cache=T,eval=FALSE,include=FALSE}
psych::ICC(sf_imb,missing=FALSE) 
```

But `ICC_lmm` will work as usual.
```{r, message=FALSE, warning=FALSE, cache=T,dependson='ICC_fn'}
sfDF_imb=sfDF%>%filter(!(subject=="S3"&judge=="J3"))
ICC_lmm(sfDF_imb) 
```
